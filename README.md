# neural-network

Neural networks implemented from scratch with NumPy that can easily achieve 96.49% validation accuracy on MNIST.

- stochastic gradient descent and back propagation (implemented in batches for speedup)
- can change # of layers, size of layer, loss function and activation per layer

  Activations: Sigmoid, Softmax, Linear and ReLU, LeakyRelU, ReLU6, "LeakyRelu6"
  
  Losses: Quadratic and CrossEntropy
  
- l2 regularization

![ regression](https://user-images.githubusercontent.com/31393977/84724482-4d784f80-af56-11ea-84e0-30ade6405217.png)
