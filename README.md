# neural-network

- Neural networks implemented from scratch with NumPy 
- can change # of layers, size of layer, loss function and activation per layer

  Activations: Sigmoid, Softmax, Linear and ReLU, LeakyRelU, ReLU6
  Losses: Quadratic and CrossEntropy
  
- 96.49% validation accuracy on MNIST
- stochastic gradient descent and back propagation (implemented with NumPy batch operations)
- l2 regularization

![regression](https://user-images.githubusercontent.com/31393977/84724482-4d784f80-af56-11ea-84e0-30ade6405217.png)

## source
http://neuralnetworksanddeeplearning.com/chap1.html
