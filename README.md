# neural-network

Neural networks implemented from scratch with NumPy that can easily achieve 96.49% validation accuracy on MNIST.

- stochastic gradient descent and back propagation (implemented in batches for speedup)
- can change # of layers, size of layer, loss function and activation per layer

  Activations: Sigmoid, Softmax, Linear and ReLU, LeakyRelU, ReLU6, "LeakyRelu6"
  
  Losses: Quadratic and CrossEntropy
  
- l2 regularization
